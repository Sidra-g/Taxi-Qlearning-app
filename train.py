{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP7796gwyzTITJf7t/vjVzU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCLqmRairvwF","executionInfo":{"status":"ok","timestamp":1739120964296,"user_tz":-300,"elapsed":21132,"user":{"displayName":"Sidra Ghaffar","userId":"13124425446006795137"}},"outputId":"04cc1177-3442-476e-b82f-6656d3a82317"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]},{"output_type":"stream","name":"stdout","text":["✅ Q-table saved successfully!\n"]}],"source":["import numpy as np\n","import gym\n","\n","# Create the environment\n","env = gym.make(\"Taxi-v3\")\n","\n","# Initialize Q-table\n","q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","\n","# Hyperparameters\n","alpha = 0.1\n","gamma = 0.9\n","epsilon = 1.0\n","epsilon_decay = 0.99\n","epsilon_min = 0.01\n","num_episodes = 10000\n","\n","# Train the Q-learning model\n","for episode in range(num_episodes):\n","    state = env.reset()\n","    done = False\n","\n","    while not done:\n","        action = env.action_space.sample() if np.random.rand() < epsilon else np.argmax(q_table[state, :])\n","        next_state, reward, done, info = env.step(action)\n","        q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state, :]))\n","        state = next_state\n","\n","    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n","\n","# Save Q-table\n","np.save(\"q_table.npy\", q_table)\n","print(\"✅ Q-table saved successfully!\")"]}]}